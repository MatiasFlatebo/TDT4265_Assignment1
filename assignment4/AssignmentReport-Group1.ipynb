{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "Fill in image of hand-written notes which are easy to read, or latex equations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "$Precision = P = \\frac{TP}{TP + FP}$\n",
    "\n",
    "$Recall = R = \\frac{TP}{TP + FN}$\n",
    "\n",
    "where TP= True positive, FP = False positive and FN= False negative\n",
    "\n",
    "True positive \n",
    "\n",
    "## task 1c)\n",
    "\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean average precision for class 1 = 0.74\n",
      "Mean average precision for class 2 = 0.62\n",
      "If both classes are for one model the mean average precision = 0.68\n"
     ]
    }
   ],
   "source": [
    "p1 = [1, 1, 1, 0.5, 0.2]\n",
    "r1 = [0.05, 0.1, 0.4, 0.7, 1]\n",
    "\n",
    "p2 = [1, 0.8, 0.6, 0.5, 0.2]\n",
    "r2 = [0.3, 0.4, 0.5, 0.7, 1]\n",
    "\n",
    "map1 = sum(p1) / len(p1)\n",
    "map2 = sum(p2) / len(p2)\n",
    "\n",
    "print(f\"Mean average precision for class 1 = {map1}\")\n",
    "print(f\"Mean average precision for class 2 = {map2}\")\n",
    "print(f\"If both classes are for one model the mean average precision = {round((map1 + map2)/2 ,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "The filtering operation is called non-maximum suppression (NMS).\n",
    "\n",
    "### Task 3b)\n",
    "False. In SSD small objects can only be detected in higher resolution layers, which are the shallowest. \n",
    "\n",
    "### Task 3c)\n",
    "The reason for using different aspect ratios:\n",
    "- By using anchors with various aspect ratios, the model becomes more flexible in capturing the diverse range of object shapes present in the dataset.\n",
    "- Using anchors with different aspect ratios helps improve the localization accuracy of the detected objects.\n",
    "- It increases the likelihood of capturing objects of various shapes during training and inference. This helps improve the recall of the detector, ensuring that a broader range of objects is detected across different scales and aspect ratios.\n",
    "\n",
    "### Task 3d)\n",
    "Main difference:\n",
    "- Object localization: SSD uses anchors of varo\\ious aspect ratios at each spatial location in the feature map to perform the prediction, while YOLO predicts bounding boxes and class probabilities for those using a single neural network. \n",
    "- Architecture: SSD and YOLO uses feedforward CNN architecture, but SSD uses multiple layers for predicting object bounding boxes and class scores while YOLO does this in a singel step using a single neural network. \n",
    "- Speed and accuracy: SSD aims to strike a balance between detection accuracy and speed. It achieves real-time performance on commodity hardware while still providing competitive accuracy for object detection tasks. YOLO prioritizes speed over accuracy, aiming for real-time object detection on limited computational resources. While it offers fast inference speed, it may sacrifice some accuracy compared to other methods like SSD.\n",
    "\n",
    "### Task 3e)\n",
    "$Total \\ number \\ of \\ anchors = Number \\ of \\ anchor \\ locations * number \\ of \\ anchors \\ per \\ location$\n",
    "\n",
    "The number of anchor locations is equal to the total number of spatial positions in the feature map, which is the product of its height and width. We have that the height and width is 38 and we have 6 achors per location.\n",
    "\n",
    "$Number \\ of \\ anchors \\ per \\ location = 38*38 = 1444$\n",
    "\n",
    "$Total \\ number \\ of \\ anchors = 1444 * 6 = 8664$\n",
    "\n",
    "### Task 3f)\n",
    "$Total \\ number \\ of \\ anchors = (38*38 + 19*19 + 10*10 + 5*5 + 3*3 + 1*1)*6 = 11640$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "FILL IN ANSWER. \n",
    "\n",
    "## Task 4c)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4d)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4e)\n",
    "FILL IN ANSWER. \n",
    "\n",
    "\n",
    "## Task 4f)\n",
    "FILL IN ANSWER. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
