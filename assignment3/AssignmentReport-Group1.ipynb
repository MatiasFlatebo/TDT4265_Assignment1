{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "<img src=\"task1a_1.png\"><br>\n",
    "<img src=\"task1a_2.png\"><br>\n",
    "<img src=\"task1a_3.png\"><br>\n",
    "<img src=\"task1a_4.png\"><br>\n",
    "<img src=\"task1a_5.png\"><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11   2   4  -1   7]\n",
      " [-24   8  12  -5  12]\n",
      " [-19  10   4  -3  15]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy \n",
    "import numpy as np   \n",
    "\n",
    "x = np.array([[2,1,2,3,1],\n",
    "              [3,9,1,1,4],\n",
    "              [4,5,0,7,0]])\n",
    "y = np.array([[-1,0,1],\n",
    "              [-2,0,2],\n",
    "              [-1,0,1]])\n",
    "print(scipy.ndimage.convolve(x,y,mode='constant'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "That is max pooling. Max pooling reduces the complexity of the image by keeping the maximum value of pixels close to each other. This makes the network more invariant to small translations nad distoritions in the input. \n",
    "\n",
    "## task 1c)\n",
    "\n",
    "To keep the dimensions:\n",
    "$Padding = \\frac{Kernel \\, size - 1}{2}$\n",
    "\n",
    "In this case kernel size is 7:\n",
    "$Padding = \\frac{7 - 1}{2} = 3$\n",
    "\n",
    "## task 1d)\n",
    "\n",
    "If the spatial dimensions of the features maps to 508x508 and no padding is used and the stride is 1, then the reduction in height and width will then be kernel size - 1. Since the reduction is 512-508=4, the kernel must be 5x5.\n",
    "Output = 508, input = 512, padding = 0, stride = 1\n",
    " \n",
    "$output = \\frac{input - kernel size + 2 * padding}{stride} + 1$\n",
    "\n",
    "$508 = 512 - K + 1 -> K = 5$\n",
    "\n",
    "## task 1e)\n",
    "Input = 508, kernel size = 2, padding = 0, stride = 2\n",
    "\n",
    "$output = \\frac{input - kernel size + 2 * padding}{stride} + 1$\n",
    "\n",
    "$output = \\frac{508 - 2}{2} + 1 = 254$\n",
    "\n",
    "## task 1f)\n",
    "\n",
    "Input = 254, kernel size = 3, padding = 0, stride = 1\n",
    "\n",
    "$output = \\frac{input - kernel size + 2 * padding}{stride} + 1$\n",
    "\n",
    "$output = \\frac{254 - 3}{1} + 1 = 252$\n",
    "\n",
    "## task 1g)\n",
    "\n",
    "params = weights + biases\n",
    "\n",
    "Params in a convolutional layer:\n",
    "\n",
    "$params = (k*k*M+1)*N$, where\n",
    "\n",
    "k - kernel size<br>\n",
    "M - number of filters in previous layer<br>\n",
    "N - number of filters in a layer<br>\n",
    "\n",
    "\n",
    "Params in a fully conected layer:\n",
    "\n",
    "$params = N*(M+1)$\n",
    "\n",
    "M - number of hidden units in previous layer<br>\n",
    "N - number of hidden units in a layer<br>\n",
    "\n",
    "\n",
    "Layer 1: $params = (5*5*3+1)*32 = 2432$<br>\n",
    "Layer 2: $params = (5*5*32+1)*64 = 51 264$<br>\n",
    "Layer 3: $params = (5*5*64+1)*128 = 204 928$<br>\n",
    "Flatten after layer 3 gives number of units = 128 * 4 * 4 = 2048<br>\n",
    "128 is number of filters in layer 3, 4 * 4 is given from the start 32x32 pixels that has been max pooled with 2x2 kernel and padding 2, three times.<br>\n",
    "Layer 4: $params = 64(2048 + 1) = 131 136$<br>\n",
    "Layer 5: $params = 10(128 + 1) = 650$<br>\n",
    "\n",
    "$Total params = 2432 + 51 264 + 204 928 + 131 136 + 650 = 390 410$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "<img src=\"plots/task2.png\">\n",
    "\n",
    "\n",
    "### Task 2b)\n",
    "<img src=\"task2_accuracy.png\">\n",
    "Training accuracy: 0.878<br>\n",
    "Validation accuracy: 0.730<br>\n",
    "Test accuracy: 0.7342"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "| Layer | Layer Type        | Number of Hidden Units / Number of Filters | Activation Function |\n",
    "|-------|-------------------|-------------------------------------------|----------------------|\n",
    "| 1     | Conv2D            | 32                                        | ReLU                 |\n",
    "| 1     | MaxPool2D         | –                                         | –                    |\n",
    "| 2     | Conv2D            | 128                                       | ReLU                 |\n",
    "| 2     | MaxPool2D         | –                                         | –                    |\n",
    "| 3     | Conv2D            | 256                                       | ReLU                 |\n",
    "| 3     | MaxPool2D         | –                                         | –                    |\n",
    "| Flatten | –               | –                                         | –                    |\n",
    "| 4     | Fully-Connected   | 128                                       | ReLU                 |\n",
    "| 5     | Fully-Connected   | 10                                        | Softmax              |\n",
    "\n",
    "For all convolutional layers:<br>\n",
    "Stride=1<br>\n",
    "Padding=2<br>\n",
    "\n",
    "Kernel for layer 1= 3x3<br>\n",
    "Kernel for layer 2= 4x4<br>\n",
    "Kernel for layer 3= 5x5<br>\n",
    "\n",
    "Optimizer: sgd<br>\n",
    "Regularization: None<br>\n",
    "Learning rate: 0.05<br>\n",
    "Batch size: 64<br>\n",
    "Weight init: None<br>\n",
    "Data augmentation: Used transforms.Lambda(random_rotate_images) and transforms.Lambda(random_color_jitter)<br>\n",
    "Added batch normalization after every convolutional layer<br>\n",
    "Early stopping: 4 epochs\n",
    "### Task 3b)\n",
    "<img src=\"plots/task3.png\">\n",
    "\n",
    "<img src=\"task3_accuracy.png\">\n",
    "\n",
    "### Task 3c)\n",
    "\n",
    "### Task 3d)\n",
    "\n",
    "### Task 3e)\n",
    "\n",
    "### Task 3f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer: adam<br>\n",
    "Batch size: 32 <br>\n",
    "Learning rate: 5*10^-4<br>\n",
    "Data augmentation: Resize til (112,112) <br>\n",
    "\n",
    "<img src=\"plots/task4.png\">\n",
    "\n",
    "<img src=\"task4_accuracy.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
